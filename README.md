# Лабораторная работа №3: Оркестрация ETL-процессов с Apache Airflow

## Бизнес-задача: Анализ результатов спортсменов и выявление топ-3 тренеров

### Описание проблемы
Спортивная федерация собирает данные о спортсменах, тренерах и результатах соревнований. Однако анализ этих данных выполняется вручную, что занимает много времени и приводит к ошибкам. Необходимо автоматизировать процесс анализа, чтобы ежедневно выявлять лучших тренеров по количеству первых мест их спортсменов.

### Бизнес-ценность решения
- 📊 **Автоматизация аналитики** - ежедневное формирование рейтинга тренеров без участия аналитика
- 🎯 **Повышение прозрачности** - объективная оценка результативности тренеров
- 📈 **Принятие решений на основе данных** - использование статистики для формирования поощрений
- 📧 **Своевременное уведомление** - автоматическая рассылка отчётов в отдел аналитики и управления

### Вариант задания №10: Спортивные результаты — топ-3 тренеров по первым местам

**Исходные данные:**
- **Файл 1 (CSV)**: athletes.csv: athlete_id, name, coach_id
- **Файл 2 (Excel)**: coaches.xlsx: coach_id, coach_name, experience_years
- **Файл 3 (JSON)**: results.json: athlete_id, competition, place

**Бизнес-логика расчета:**

1. Отфильтровать спортсменов, занявших 1 место
2. Соединить данные спортсменов и тренеров
3. Подсчитать количество первых мест у каждого тренера
4. Отсортировать и выбрать топ-3

**Ожидаемый результат:** Аналитический отчёт с топ-3 тренерами и количеством первых мест, а также автоматическая рассылка отчёта по email.

## Архитектура решения

### Общая схема системы

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                           DOCKER COMPOSE ENVIRONMENT                         │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐          │
│  │   PostgreSQL    │    │   Apache        │    │    MailHog      │          │
│  │   Database      │    │   Airflow       │    │  Email Service  │          │
│  │ Port: 5432      │    │ Port: 8080      │    │ SMTP: 1025      │          │
│  │ DB: airflow     │    │ DAGs scheduler  │    │ Web: 8025       │          │
│  └─────────────────┘    └─────────────────┘    └─────────────────┘          │
│                                                                              │
│                         ┌─────────▼─────────┐                               │
│                         │   DAGs Volume     │                               │
│                         │ ./dags:/opt/airflow│                              │
│                         └───────────────────┘                               │
└──────────────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────────────┐
│                                HOST SYSTEM                                   │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐   ┌─────────────────┐   ┌──────────────────────────┐    │
│  │   Data Files    │   │   DAG Files     │   │  Result Files            │    │
│  │ athletes.csv    │   │ coaches_top3_   │   │ top_coaches.db           │    │
│  │ coaches.xlsx    │   │ dag.py          │   │ top_coaches_report.txt   │    │
│  │ results.json    │   │                 │   │ top_coaches_data.csv     │    │
│  └─────────────────┘   └─────────────────┘   └──────────────────────────┘    │
└──────────────────────────────────────────────────────────────────────────────┘

```

### ETL-процесс для анализа удержания пользователей

```
                     EXTRACT PHASE (Параллельно)
┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐
│ extract_athletes │  │ extract_coaches  │  │ extract_results  │
│ athletes.csv     │  │ coaches.xlsx     │  │ results.json     │
└──────────────────┘  └──────────────────┘  └──────────────────┘
          │                  │                   │
          └──────────────────┼───────────────────┘
                             │
                     TRANSFORM PHASE
                 ┌────────────────────┐
                 │ transform_data     │
                 │                   │
                 │ 1. JOIN таблицы   │
                 │ 2. FILTER place=1 │
                 │ 3. COUNT per coach│
                 └────────────────────┘
                             │
                     LOAD PHASE
                 ┌────────────────────┐
                 │ load_to_database   │
                 │ SQLite: top_coaches│
                 └────────────────────┘
                             │
                     REPORT PHASE
                 ┌────────────────────┐
                 │ generate_report    │
                 │ TXT + CSV файлы    │
                 └────────────────────┘
                             │
                   NOTIFICATION PHASE
                 ┌────────────────────┐
                 │ send_email_with_   │
                 │ results            │
                 │ HTML + Attachments │
                 └────────────────────┘
```

## Технический стек

| Компонент | Технология | Назначение |
|-----------|------------|------------|
| **Оркестрация** | Apache Airflow 2.5.0 | Управление ETL-процессами |
| **Контейнеризация** | Docker + Docker Compose | Изоляция и развертывание |
| **База данных (Airflow)** | PostgreSQL 12 | Метаданные Airflow |
| **База данных (Результаты)** | SQLite | Хранение результатов анализа |
| **Обработка данных** | Python 3.8 + Pandas | ETL-логика и трансформации |
| **Работа с Excel** | openpyxl | Чтение Excel файлов |
| **Email-тестирование** | MailHog | Тестирование email-уведомлений |
| **Веб-интерфейс** | Airflow WebUI | Мониторинг и управление |

## Пошаговая инструкция запуска

### Шаг 1: Подготовка окружения

1. **Клонирование проекта**:
   ```bash
   git clone https://github.com/BosenkoTM/DCCAS.git
   cd DCCAS/lw_03
   ```

2. **Запуск всех сервисов**:
   ```bash
   sudo docker compose up -d
   ```

3. **Проверка статуса контейнеров**:
   ```bash
   sudo docker ps
   ```
   Должны быть запущены: `webserver`, `scheduler`, `postgres`, `mailhog`

### Шаг 2: Работа с Apache Airflow

#### Доступ к веб-интерфейсу
- **URL**: http://localhost:8080
- **Логин**: admin
- **Пароль**: admin

#### Что смотреть в Airflow UI:

1. **Главная страница (DAGs)**:
   - Найдите DAG `mobile_apps_retention_analysis`
   - Убедитесь, что он включен (переключатель слева должен быть активен)

2. **Запуск DAG**:
   - Кликните на название DAG
   - Нажмите кнопку "Trigger DAG" (▶️) для ручного запуска
   - Или дождитесь автоматического запуска по расписанию

3. **Мониторинг выполнения**:
   - **Graph View** - визуальное представление задач и их зависимостей
   - **Tree View** - история запусков DAG
   - **Gantt View** - временная диаграмма выполнения задач

4. **Статусы задач**:
   - 🟢 **Success** - задача выполнена успешно
   - 🔴 **Failed** - задача завершилась с ошибкой
   - 🟡 **Running** - задача выполняется
   - ⚪ **Queued** - задача в очереди на выполнение

5. **Просмотр логов**:
   - Кликните на задачу в Graph View
   - Выберите "View Log" для просмотра детальных логов

### Шаг 3: Проверка email-уведомлений в MailHog

#### Доступ к MailHog
- **URL**: http://localhost:8025

#### Что смотреть в MailHog:

1. **После успешного выполнения DAG** появится письмо:
   - **От**: airflow@example.com
   - **Кому**: test@example.com
   - **Тема**: "📊 Анализ коэффициента удержания мобильных приложений - Результаты"

2. **Содержимое письма**:
   - 📊 HTML-таблица с результатами анализа по категориям
   - 📎 Прикрепленные файлы:
     - `retention_analysis_report.txt` - подробный отчет
     - `retention_analysis_data.csv` - данные в формате CSV

3. **Интерфейс MailHog**:
   ```
   ┌─────────────────────────────────────────┐
   │  MailHog Web Interface                  │
   │  http://localhost:8025                  │
   ├─────────────────────────────────────────┤
   │  📧 Inbox (1)                          │
   │  ┌─────────────────────────────────────┐ │
   │  │ ✉️  Анализ коэффициента удержания   │ │
   │  │     airflow@example.com             │ │
   │  │     2024-10-02 15:30:45            │ │
   │  └─────────────────────────────────────┘ │
   │                                         │
   │  📄 Message Preview:                    │
   │  HTML-таблица с результатами           │
   │  📎 Attachments: 2 files               │
   └─────────────────────────────────────────┘
   ```

### Шаг 4: Проверка результатов анализа

#### Использование скрипта check_results.py

1. **Установка зависимостей**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Проверка результатов в базе данных**:
   ```bash
   python3 check_results.py
   ```
   Скрипт автоматически:
   - Найдет контейнер scheduler
   - Скопирует базу данных SQLite из контейнера
   - Покажет результаты анализа в удобном формате

3. **Копирование файлов результатов**:
   ```bash
   python3 check_results.py --files
   ```
   Скопирует из контейнера:
   top_coaches_report.txt
   top_coaches_data.csv

4. **Справка по скрипту**:
   ```bash
   python3 check_results.py --help
   ```

#### Пример вывода результатов:

```
Анализ спортивных результатов завершён успешно
DAG: coaches_top3_analysis

Дата выполнения: 2025-11-11

Топ-3 тренеров по количеству первых мест:

ОТЧЕТ: Топ-3 тренеров по количеству первых мест
===============================================================
Дата анализа: 2025-11-12 18:30:03

1. Никитин Алексей Юрьевич — 3 первых мест
2. Новиков Михаил Игоревич — 3 первых мест
3. Волков Андрей Павлович — 2 первых мест
```

### Шаг 5: Остановка сервисов

```bash
sudo docker compose down
```

## Полная очистка проекта

### Автоматическая очистка с помощью cleanup.sh

Используйте готовый скрипт для полной очистки Docker окружения:

```bash
# Сделать скрипт исполняемым
chmod +x cleanup.sh

# Запустить полную очистку
sudo ./cleanup.sh
```

**Что делает скрипт cleanup.sh:**
- ✅ Останавливает все контейнеры
- ✅ Удаляет все контейнеры проекта
- ✅ Удаляет образы проекта (Apache Airflow, PostgreSQL, MailHog)
- ✅ Удаляет все тома и сети Docker
- ✅ Очищает систему Docker от неиспользуемых ресурсов
- ✅ Удаляет локальные файлы результатов
- ✅ Показывает подробную статистику очистки
- ✅ Запрашивает подтверждение перед выполнением

**Использование:**
```bash
sudo ./cleanup.sh
```

Скрипт интерактивный и безопасный - запросит подтверждение перед удалением данных.

## Структура проекта

```
lw_03/
├── dags/
│   ├── data/                          # Исходные данные
│   │   ├── athletes.csv               # Приложения и категории (20 записей)
│   │   ├── coaches.xlsx               # Данные об установках (20 записей)
│   │   └── results.json               # Данные об удалениях (20 записей)
│   ├── 01_umbrella.py                 # Пример DAG (оригинальный)
│   ├── aggreg.py                      # Пример агрегации (оригинальный)
│   └── coaches_top3_dag.py            # DAG для варианта №30 ⭐
├── docker-compose.yml                 # Конфигурация Docker Compose
├── requirements.txt                   # Зависимости Python
├── check_results.py                   # Скрипт для проверки результатов 🔍
├── cleanup.sh                         # Скрипт полной очистки 🧹
└── README.md                          # Данная инструкция
```

## Результаты выполнения

После успешного выполнения DAG вы получите:

1. **📊 Аналитический отчет** с расчетом коэффициента удержания по категориям
2. **📧 Email-уведомление** с HTML-таблицей результатов и прикрепленными файлами
3. **📁 Файлы результатов**:
   - `retention_analysis_report.txt` - подробный текстовый отчет
   - `retention_analysis_data.csv` - данные в формате CSV для дальнейшего анализа
   - `mobile_apps_retention.db` - база данных SQLite с результатами
4. **📈 Бизнес-рекомендации** на основе анализа данных


## Выводы

### Технические достижения
✅ Реализован полный ETL-процесс в среде Apache Airflow для автоматического анализа спортивных данных.

✅ Автоматизирована обработка разнородных источников (CSV, Excel, JSON) с объединением данных спортсменов, тренеров и результатов соревнований.

✅ Настроена оркестрация задач с корректными зависимостями и параллельным выполнением стадий извлечения данных.

✅ Реализована автоматическая генерация отчётов в формате TXT и CSV.

✅ Внедрена система уведомлений — отправка итогов анализа по email через MailHog.

✅ Обеспечена устойчивость к ошибкам благодаря механизму повторных попыток и логированию на каждом этапе.

### Бизнес-результаты
1. 📊 **Автоматизирован расчет ключевых метрик** результативности тренеров
2. 🎯 **Создана прозрачная система рейтинга тренеров** с различными показателями эффективности
3. 📈 **Оптимизирвоан процесс аналитики** 
4. ⏰ **Сокращено время** на подготовку аналитических отчетов
5. 📧 **Обеспечена своевременная доставка** результатов заинтересованным лицам 

### Практические навыки
- Проектирование и реализация ETL-процессов
- Работа с Apache Airflow (DAGs, операторы, зависимости)
- Контейнеризация приложений с Docker Compose
- Обработка данных с помощью Python и Pandas
- Настройка email-уведомлений и мониторинга
- Работа с различными форматами данных (CSV, Excel, JSON, SQLite)

### Возможности для развития
- Добавление более сложных аналитических метрик
- Интеграция с реальными источниками данных (API, базы данных)
- Настройка алертов при критических изменениях показателей
- Создание дашбордов для визуализации результатов
- Масштабирование на кластер Airflow для больших объемов данных

Данная лабораторная работа демонстрирует полный цикл создания производственного ETL-решения с использованием современных инструментов оркестрации данных.
